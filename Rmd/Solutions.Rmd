---
title: " Exploring Solutions for extracting recovery measures information from Recovery Documents using Reproducible Analytical Pipelines"
author: "Strategic RAP Champions [Quentin Stoyel & Catalina Gomez]"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(DiagrammeR)

library(reticulate)
use_python("../venv/Scripts")

```

## Project Conceptual Diagram 

This document summarizes a collection of solutions for extracting recovery measures information from Recovery Documents using Reproducible Analytical Pipelines. Findings will help identify best practices for future data entry and reporting. Outcomes of this project will include: 

- Testing and evaluating the use of Reproducible Analytical Pipelines to automate data extraction from SARA recovery documents and Species at Risk geodatabase.  

- Make recommendations regarding document elements (e.g. formatting or codes) required to identify and extract relevant information into a spreadsheet. This will coincide with the Species at Risk Program recovery and implementation team plans to revise Species’ Progress Report templates and other recovery document templates this fiscal year. 

- Conceptual workflow of elements that would be required to reverse-engineer outcomes 1 and 2 (above) by using forms, csv files, or Excel spreadsheets to generate reproducible reports (e.g. using Microsoft PowerBI, R Markdown, or other tools that are easily used and accessible to Species at Risk Program staff). 

For more information about this project, please see the [project proposal.](https://086gc.sharepoint.com/:w:/s/MaritimesSpatialPlanning-MAROpenDataLogistics/EZLu-2jTNt1DgVY_0GsX3eEBtsJwi7xA_vWNSAqwBURbdg) 

```{r workflow, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
workflow <- DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  rec1 [label = 'Step 1. Recovery Document Table [PDF, Word, or...]']
  rec2 [label = 'Step 2. Data Frame [In R, Python, or ...]']
  rec3 [label = 'Step 3. File [csv, excel or...]']
  rec4 [label = 'Step 4. Reporting [interactive reporting web-based and customized static reporting]']
  
  rec1 -> rec2 -> rec3 -> rec4
  }",
  height = 500)
```

```{r workflow plot}
workflow
```

## Exploration of options for Step 1 to Step 2: Examples below display outcomes for Table 4 of Mud-Piddock Recovery Assessment.

```{r workflow2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Option1 <- DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  rec1 [label = 'Table in PDF/Word of selected Recovery Documents']
  rec2 [label = 'Tool e.g. R, Python, and others']
  rec3 [label = 'Table Extraction']
  rec4 [label = 'Data Frame']

  rec1 -> rec2 -> rec3 -> rec4 
  }",
  height = 500)
```

```{r workflow plot2}
Option1
```
This first section provides a summary of solutions if tables in PDFs are the preferred way to store information. Examples below display outcomes for Table 4 of Mud-Piddock Recovery Assessment. 

![Original screenshot of table in the PDF](table4.png)


### `tabulizer` R Package

tabulizer provides R bindings to the Tabula java library, which can be used to computationaly extract tables from PDF documents. Package ‘tabulizer’ was removed from the CRAN repository thus installation was conducted directly from GitHub. This could leave us at the mercy of package developers that could discontinue its maintenance. More here: https://github.com/ropensci/tabulizer 

```{r}
#remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"), INSTALL_opts = "--no-multiarch")
```

```{r}
library(remotes)
library(rJava)
library(tabulizer)
tabletest <- "Rs-PholadeTronqueeAtlMudPiddock-v00-2022Aug-Eng.pdf"
#fullextraction <- extract_tables(tabletest) # You may uncomment this line if you would like to extract tables form the whole document.
table4 <- extract_tables(tabletest, pages = 20)
table4
```

The output is a bit convoluted with a series of empty spaces and disjointed/unconnected text. Thus next step is to turn the table extraction into a data frame to get an output that resembles the original table a tiny bit more, yet issues prevailed and a bit more tweaking is required (coding side and/or way tables are designed = the simpler the table, the easier for the code to extract it).  

```{r}
table4df <- extract_tables(tabletest, pages = 20, output = "data.frame") %>% as.data.frame()
table4df
```

The solution below still requires a bit more development but the simple extraction seems similar to the original table 4. 

```{r}
library(kableExtra)
table4df %>% kbl(caption = "Table 4") %>%
         kable_classic(full_width = F, html_font = "Cambria") 
```

*Demos*:

- https://github.com/ropensci/tabulizer
- Tabulizer Package: https://www.youtube.com/watch?v=nlsWjezvsg8
- PDF Scrape and exploratory analysis code demo: https://www.business-science.io/code-tools/2019/09/23/tabulizer-pdf-scraping.html#workflow

### `docxtractr` R Package

PDFs seem to have some tricky issues consistently observed while trying different solutions. What about Word Files? We saved Table 4 as a separate Word file and tested `docxtractr` R Package. 

![screenshot of table in Word is displayed above.](table4-simplified-word.png)

The solution below still requires a bit more development but the simple extraction seems similar to the original table 4. Keeping tables as simple as possible in the word or PDF file is one of the main pieces to implement moving forward. 

```{r}
library(docxtractr)
tabletestdocx <-  read_docx("table4.docx")
docx_tbl_count(tabletestdocx)
tbls <- docx_extract_all_tbls(tabletestdocx)
tbls
```

```{r}
library(kableExtra)
tbls %>% kbl(caption = "Table 4") %>%
         kable_classic(full_width = F, html_font = "Cambria") 
```

### `Camelot` Python Package
Camelot is a python package, installable from Pip that can extract tables from a PDF based on the lines between cells (Lattice).  This helps improve the quality of the parsed data.

```{python camelot_example}
from camelot import read_pdf

doc_file_path = "Rs-PholadeTronqueeAtlMudPiddock-v00-2022Aug-Eng.pdf"
df_list = read_pdf(doc_file_path, pages="20", line_scale=50)
# this is a list, because there might be more than one table on the page
# print everything:
print(df_list[0].df.to_string())
```
There are a couple potential issues with this method.  Special characters are explitly recorded (newlines are saved as \\n) and items such as graphs can be mistaken as tables.



### `Docx` Python Package
Docx is a python package, installable from Pip (`pip install python-docx`) that can extract tables from word documents.

```{python docx_example}
from docx import Document
recovery_docx = Document("Rs-PholadeTronqueeAtlMudPiddock-v00-2022Aug-Eng.docx")
# recovery_docx.tables is a list containing all of the tables in the docx, print the 3rd one:
for row in recovery_docx.tables[3].columns:
    for cell in row.cells:
        print(cell.text)

```
This method produces a very reliable output that is fairly easy to work with.

### Tabula

- Tabula is free and open source. Tabula was created by journalists for journalists and anyone else working with data locked away in PDFs. https://tabula.technology/  
Software can be used to extract data manually, or folks can implement a solution programatically using python.

*Demos*: 

- https://www.youtube.com/watch?v=702lkQbZx50
- https://www.youtube.com/watch?v=NvcTIZ2Je50 


### Azure Form Recognizer

[Azure Form Recognizer](https://docs.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/overview?WT.mc_id=aiml-14201-cassieb&tabs=v3-0) is a cloud-based Azure Applied AI Service that analyzes forms and documents, extracts text and data, and maps field relationships as key-value pairs: https://formrecognizer.appliedai.azure.com/studio

Azure Form Recognizer could potentially allow coordination with tools withing our Microsoft Suite. It is powered by Python. 

Azure Form Recognizer does not exist in our current suite of tools. We logged in a ticket with IT and they confirmed that in order to complete our request we would need you fill out a form so that software can be evaluated by IT security before installing to make sure it is safe to have it on our network. It does not seem to be an open source solution (0-500 pages are free per month, after that, there is a cost associated with using this tool https://azure.microsoft.com/en-ca/pricing/details/form-recognizer/)

We have not tested this option yet as there seems to be a price tag associated with number of pages depending on the capacity of data mining. 

*Demos*:

  - https://www.youtube.com/watch?v=rkJa6vbkMcU
  - https://github.com/Azure-Samples/cognitive-services-quickstart-code/blob/master/python/FormRecognizer/rest/python-train-extract.md


## Implementing solutions for Step 1 to Step 2: Examples below display outcomes for two Word Documents slighly modified

```{r}
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  rec1 [label = 'PDF Recovery Documents']
  rec2 [label = 'Create Word Recovery Document using Foxit']
  rec3 [label = 'Identify key terms of tables to be extracted in the Word document']
  rec4 [label = 'Test extraction of relevant tables']

  rec1 -> rec2 -> rec3 -> rec4 
  }",
  height = 500)
```

Word files seem to be providing a really nice output. Our next task was to explore which tables specifically will be mined. For this, we manually compared the [spreadsheet information](https://086gc.sharepoint.com/:x:/s/NatureLegacyDatabase/ETYmtFpJ9AZOvNfXyHrAPTABOdFZi_XDRDUuZaIqSHo8tA) shared by the SARA Program and the source document/word files for Atlantic Mud-piddock and Blue whale. 

Column Name in Word Doc Table    | Column Name in Spreadsheet
-------------                    | -------------       
Species                          | Species Name         
Threat                           | Not included         
Geographic scale                 | Not included      
Likelihood of occurrence         | Not included      
Level of impact                  | Not included      
Causal certainty                 | Not included
Threat risk                      | Not included
Threat occurrence                | Not included
Threat frequency                 | Not included
Threat extent                    | Not included
General description of research and management approaches | Recovery Measures/ Conservation Measures  
Priority                         | Not included
Broad strategy                   | Not included
Threat or concern addressed      | Not included


Column Name in Spreadsheet       | Column Name in Word Doc Table 
-------------                    | -------------       
Designatable Unit (DU)           | Not included         
Taxon	                           | Not included      
COSEWIC Status                	 | Not included      
SARA Status	                     | Not included 
Lead Region                      | Not included 
Broad Strategy                   | Not included 
Document Used	                   | Not included 
Document Reference               | Not included 


Column name in Spreadsheet Tab Recovery Measure  | Column name in Spreadsheet Tab Recovery Measure Tracking
-------------                                    | -------------       
Recovery Measures/ Conservation Measures         | AP: Recovery Measures

We tested the process of adding a potential new table in the Recovery Documents/Action Plans that could be added in future word files with information that could be mined into an authoritative spreadsheet. 

![Proposed new table](proposed-new-table.png)

We also modified and standardized names of the additional columns to be mined from the word documents into the spreadsheets. These are tentative names until SARA Program decides which names could be used systematically in all future documents. This will make the process of mining tables significantly simpler as the codes will be looking for this unique names within tables. For this test, we did the following changes in the two word documents: 

- The symbol "#" to indicate the number of the Recovery Measure was replaced with "Measure ID" 
- "General description of research and management approaches" in Atlantic mud-piddock and "Recovery measures" in Blue Whale was replaced with the column name "Detailed Measures"
- "Measure ID" and "Detailed Measures" were combined in the blue whale document; we separated them into two columns for the blue whale document to match the requirements in the spreadsheet and the mud-piddock document.
- Values in the "Broad strategy" column were replaced with text instead of code numbers, to match the blue whale recovery document and to make the process of mining info as simple as possible e.g. "1" was replaced with "Research and monitoring"
 
The above mentioned changes have led to the following key words to mine the two test word documents:

- Measure ID
- Detailed Measures
- Broad strategy
- Species Name
- Designatable Unit	
- Taxon	
- COSEWIC Status
- SARA Status	
- Lead Region	
- Document Used



Note: solutions will not be case sensitive.


## Recommendations:
The exploration performed above has produced the following recommendations for extracting tables from recovery documents:
- While tables can be extracted from both Word and PDF documents, Word documents produce accurate results with far fewer formating requierements.
- Python and R are both viable options to perform the extraction.
- The metadata fields should also be included in the scraping process, possibly in the form iof user input or a metadata table added to the document.

Additionally, the following recomendations are made for the actual formating of the tables inside the recovery documents:
- Tables rows do not break from page to page
- Columns names are unique, short, succinct, and fully standardized in *all* recovery documents
- Unique column names are KEY because code will be looking for those in the tables extracted
- Do not use numbers/codes in the values of tables. Please use text to make the mining process as simple as possible.e.g. instead of using "1" for Broad strategy, use the direct wording "Research and monitoring"


Mining tables from Word files proved to be the most effective solution using the least amount of coding and manipulation. We briefly tested one of the documents with the new proposed tables. 

```{r}
library(docxtractr)
testdocx <-  read_docx("Rs-PholadeTronqueeAtlMudPiddock-v00-2022Aug-Eng.docx")
docx_tbl_count(testdocx)
alltables <- docx_extract_all_tbls(testdocx)
str(alltables)
```

```{r}
library(kableExtra)
alltables[[1]] %>% kbl(caption = "Testing Full Extraction of Tables") %>%
         kable_classic(full_width = F, html_font = "Cambria") 
```

## User Stories

DFO staff is tasked with mining tables from Recovery Documents PDF/Word files to organized them in a nice and clean spreadsheet for future reporting. Below are potential future scenarios of what their experience could be using different approaches.
  
####  Scenario 1: Staff mines tables directly from current Recovery Document PDF/Word files

Staff mine tables that have weird double lines, non matching lines and background white lines separating the cells. Significant amount of manual intervention, extra coding and tweaking is required for each table in a customized fashion. This scenario is a bit cumbersome and requires extra manipulation.    

#### Scenario 2: Staff mines tables directly from improved Recovery Document Word files that include additional metadata (a.k.a. tables). 

Using a Reproducible Analytical Pipeline, staff mine tables directly from improved Word files provided by the authors, or generated from the authoritative PDFs using software like Foxit. Improved Word documents follow explicit guidance to create metadata (a.k.a. tables) that are mined at the same time as the filepath when scraping the document. Staff mine tables efficiently and easily because authors rigorously applied the following recommendations for each table:

- Tables rows do not break from page to page
- Columns names are unique, short, succinct, and fully standardized in *all* recovery documents  
- Unique column names are KEY because code will be looking for those in the tables extracted 
- Tables do not include numbers/codes in the values of tables. Instead, tables include standardized text to make the mining process as simple as possible e.g. instead of using "1" for coding Broad strategies, staff use the direct wording "Research and monitoring".

#### Scenario 3: Staff takes advantage of a new workflow to store and mine the data using a GUI

Mining PDF and Word files is a possibility. Is there a better approach for mining data and storing it in an authoritative place? Data could be mined from something different than a PDF/Word file. Something such as database, excel sheets, GUI, which could be populated as part of a submission. In this case, standardized tables are not a deal breaker for mining information.

The evaluation of any scenario must take into account the time and user experience of i) folks writing/publishing SARA documents, ii) folks mining these documents, iii) Staff needed to maintain new tools.

### Recommendations for input spreadsheet 

Need a bit of cleaning before interactive report pilot - happy to do cleaning with recommendations on how to keep spreadsheets as clean as possible for coding (Excel spreadsheets, CSV, Databases)

## Flags & Questions TBD 

  - Are PDFs or Word Files the best way to store metadata (a.k.a) tables from Recovery Documents?
    - Word seems to work great particularly if very specific conditions are applied for every future publication. This could increase time folks spend formatting. 
- When mining tables in the future from the Recovery Document PDF/Word file:
   - Do you envision mining tables directly from the Recovery Document PDF/Word file directly? We could slect one of the options presented in this document to explore further.
   - Or are you open to recommendations in the workflow to store and mine the data from somewhere else (e.g. database, excel sheets populated as part of submission)?
